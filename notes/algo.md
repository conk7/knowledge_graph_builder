# структура репозитория

/MyObsidianVault

├── 00_Inbox/

│ // Папка для всех новых, необработанных файлов.

│ // Алгоритм в первую очередь смотрит сюда.

│ ├── 10_Notes/

│ // Основная база знаний. Обработанные, атомарные заметки.

│ // Может содержать тематические подпапки, которые алгоритм поможет поддерживать.

│ ├── AI/

│ │ └── Concept_of_Embeddings.md

│ └── Economics/

│ ├── 20_Tags/

│ // Папка с файлами-тегами. Это пустые .md файлы.

│ // Их ИМЕНА - это и есть теги.

│ └── Artificial [Intelligence.md](http://Intelligence.md)

│ └── Machine [Learning.md](http://Learning.md)

│ └── [Thesis.md](http://Thesis.md)

│ ├── 90_Fleeting/

│ // Временные заметки, списки дел, напоминания.

│ // Эта папка будет ПОЛНОСТЬЮ ИГНОРИРОВАТЬСЯ алгоритмом.

│ └── .ai_meta/

```
// Скрытая папка для данных алгоритма (не трогать руками).

└── index.faiss

└── metadata.json
```

набросок алгоритма:

1. перевести файлы в embeddings с помощью **Sentence-BERT**. файл будет разделен на чанки (например, по абзацам), для каждого из них будет embedding, файл будет представлен как кортеж embeddingов, сравнение с другим будет как попарное сравнение embeddings. Сохраните соответствие `(file_id, chunk_id, chunk_text, embedding)`.
    
2. **Поиск по ближайшим соседям (Approximate Nearest Neighbor - ANN).** После получения эмбеддингов для всех чанков, индексируйте их с помощью библиотек типа **FAISS** (от Facebook AI) или **Annoy** (от Spotify).
    
    - **Как это работает:** Вместо того чтобы делить все на жесткие кластеры, вы для _каждого_ чанка мгновенно находите N самых похожих на него чанков из всей базы данных. Это решает проблему "мультитематических" файлов и гораздо быстрее, чем попарное сравнение.
        
    - Используйте ANN-индекс, чтобы найти K-наиболее похожих на него чанков.
        
    - Это формирует ваш список пар-кандидатов для анализа `(chunk_A, chunk_B)`.
        
3. Семантические связи. **Идея:** Не все "похожие" по косинусу чанки имеют осмысленную семантическую связь. Чтобы не тратить ресурсы мощной LLM зря, используйте двухэтапный подход
    
    - **Этап 1 (Быстрая проверка):** Используйте более простую и быструю модель (или даже классификатор, обученный на парах текстов) с простым вопросом: `"Существует ли между этими двумя текстами какая-либо из следующих связей: [список]? Ответь ДА или НЕТ"`.
        
    - **Этап 2 (Глубокая классификация):** Только если ответ "ДА", отправляйте эту пару в вашу основную, мощную LLM с подробным промптом для определения _типа_ связи.
        
    - Откажитесь от идеи найти `contradicts` через косинусное сходство. Вместо этого, добавьте этот тип связи в ваш промпт для LLM. Модель сможет семантически определить противоречие.
        
    - Добавьте в ваш список связей вариант **`no_relation`** или **`similar_but_no_specific_relation`**. Это поможет отсеять ложные срабатывания, когда LLM пытается "придумать" связь там, где ее нет.
        
4. **Построение графа:**
    
    - Результат от LLM — это классифицированная связь между двумя чанками.
        
    - Агрегируйте связи. Это можно делать по частоте (самые частые связи между чанками из двух файлов) или с помощью LLM.
        
    - **Нейминг кластеров:**
        
        Для именования "групп" можно взять чанки, которые являются "центрами" в
        
        ANN-поиске (имеют много соседей), и попросить LLM дать им общее
        
        название/тему.
        

#### **Логика алгоритма**

**Шаг 1: Инициализация и загрузка состояния**

- При запуске скрипт ищет в корне хранилища скрытую папку `.obsidian_ai_meta`.
    
- **Если папка существует:** Загружает в память FAISS-индекс (`index.faiss`) и словарь с метаданными (`metadata.json`).
    
- **Если папки нет (первый запуск):** Создает пустой индекс и пустой словарь метаданных.
    

**Шаг 2: Обнаружение изменений**

- Скрипт рекурсивно сканирует все `.md` файлы в хранилище.
    
- Для каждого найденного файла вычисляется хэш-сумма его содержимого.
    
- Сравнивая текущие файлы и их хэши с теми, что хранятся в `metadata.json`, формируются три списка:
    
    - `files_to_add`: Новые файлы, которых нет в метаданных.
        
    - `files_to_update`: Файлы, чей хэш изменился (были отредактированы).
        
    - `files_to_remove`: Файлы из метаданных, которые были удалены из хранилища.
        

**Шаг 3: Обновление поискового индекса**

- **Удаление:** Для каждого файла из `files_to_remove` и `files_to_update`, скрипт находит ID его _старых_ чанков в метаданных и удаляет их векторные представления из FAISS-индекса.
    
- **Добавление:** Для каждого файла из `files_to_add` и `files_to_update`:
    
    1. Текст разбивается на осмысленные чанки (абзацы).
        
    2. Для каждого чанка генерируется эмбеддинг (вектор) с помощью Sentence-BERT.
        
    3. Новые векторы добавляются в FAISS-индекс.
        
    4. Информация о новых файлах, их чанках и хэшах записывается в словарь метаданных.
        

**Шаг 4: Поиск и классификация связей**

- **Ключевой момент:** Этот шаг выполняется **только для новых/обновленных чанков**.
    
- Для каждого _нового_ чанка (из `files_to_add`/`files_to_update`):
    
    1. С помощью FAISS-индекса находится `K` наиболее семантически близких чанков из **всего** хранилища (включая старые и новые файлы).
        
    2. Каждая найденная пара (`новый_чанк`, `похожий_чанк`) отправляется в LLM (например, Gemini).
        
    3. LLM получает промпт вида: _"Определи тип семантической связи от Документа А к Документу Б из списка [is_example_of, contradicts, ...]. Текст А: [...], Текст Б: [...]"_
        

**Шаг 5: Построение и обновление графа**

**5a. Анализ и запись связей:**

- На основе ответа от LLM скрипт генерирует Markdown-ссылку.
    
- **Агрегация:** Если между `Файл_А` и `Файл_Б` найдено несколько связей на уровне чанков, они агрегируются в одну или несколько связей на уровне файлов (например, по самой частой связи).
    
- Скрипт открывает соответствующий `.md` файл и дописывает в конец (например, в раздел `### Связи`) сгенерированную ссылку: `* Является примером для: [[Файл_Б]]`.
    
    **5b Проверка тегов и принятие решения:**
    
- Алгоритм берет тег с самой высокой уверенностью (например, "AI", уверенность 0.92).
    
- **Если уверенность < 0.8 (низкая):**
    
    - Файл перемещается в `10_Notes/00_Uncategorized/`.
        
    - Сообщение в лог: `[INFO] Файл 'Note.md' имеет неоднозначные теги. Перемещен в Uncategorized для ручной проверки.`
        
    - Процесс для этого файла завершен.
        
- **Если уверенность >= 0.8 (высокая):**
    
    - Алгоритм переходит к следующему шагу.

**5c. Проверка критической массы и создание папки:**

- Алгоритм обновляет счетчик для тега "AI" в `metadata.json`. Допустим, счетчик стал равен `3`.
    
- Он проверяет: `счетчик (3) >= FOLDER_CREATION_THRESHOLD (3)`? **Да.**
    
- Он проверяет: существует ли папка `10_Notes/AI/`? **Нет.**
    
- **Действие:** Алгоритм создает папку `10_Notes/AI/`.
    

**5d. Финальное перемещение:**

- Файл `Note.md` перемещается из `00_Inbox/` в `10_Notes/AI/`.

**5e. "Уборка" (Self-Healing Maintenance):**

- Раз уж папка для "AI" была только что создана, алгоритм может проявить инициативу.
    
- Он сканирует папку `10_Notes/00_Uncategorized/`, находит там другие файлы с тегом "AI" и **также перемещает их** в новую папку `10_Notes/AI/`.
    
- Сообщение в лог: `[INFO] Создана новая тематическая папка '10_Notes/AI/'. Перемещено 3 файла.`
    

**Шаг 6: Сохранение состояния**

- Обновленный FAISS-индекс (`index.faiss`) и словарь метаданных (`metadata.json`) сохраняются на диск в папку `.obsidian_ai_meta`.
    
- Работа завершена до следующего запуска.

### Links
* Похож на: [[datasets]]
