Осенью 2018 года команда Google AI представила BERT (Devlin et al., arXiv:1810.04805) — первую модель, которая доказала, что глубокое двунаправленное предобучение на огромных корпусах даёт универсальные представления языка, превосходящие всё, что было до этого.

### Ключевая интуиция
До BERT доминировали left-to-right модели (ELMo, GPT-1): слово видело только предыдущий контекст. BERT предложил одновременно смотреть в обе стороны, что радикально важно для задач понимания.

### Две задачи предобучения
1. **Masked Language Modeling (MLM)**  
   Случайно маскируется 15 % токенов; модель учится предсказывать их по полному контексту. Именно это даёт настоящую двунаправленность.

2. **Next Sentence Prediction (NSP)**  
   Учит модель понимать отношения между предложениями. Позже (RoBERTa, ALBERT) было показано, что NSP помогает слабо и его можно убрать.

### Архитектурные варианты
- BERT-base: L=12, H=768, A=12, 110 М параметров
- BERT-large: L=24, H=1024, A=16, 340 М параметров

### Результаты октября 2018 года
Впервые одна модель одновременно побила рекорды на 11 различных NLP-задачах:
- GLUE: 80.5 % (large)
- SQuAD v1.1: 93.2 F1
- SQuAD v2.0: 83.1 F1

Это был момент, когда сообщество окончательно признало: эпоха feature engineering в NLP завершена.

### Эволюция и критика (2019–2025)
- RoBERTa (2019) — убрала NSP, увеличила батчи и данные → +3–5 % на всех бенчмарках
- ALBERT, DistilBERT — лёгкие версии
- DeBERTa, ELECTRA — более эффективные задачи предобучения
- К 2025 году BERT-like encoder’ы всё ещё используются в поисковых системах (Google, Yandex), ранжировании, классификации и как мощные sentence encoders

BERT не стал самой большой моделью, но стал эталоном того, как правильно делать предобучение.

См. также: [[Transformer]], [[Attention]]