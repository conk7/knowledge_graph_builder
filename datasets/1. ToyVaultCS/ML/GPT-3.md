В мае 2020 года OpenAI опубликовала статью «Language Models are Few-Shot Learners» (Brown et al., arXiv:2005.14165) и одновременно выпустила модель на 175 миллиардов параметров — GPT-3. Это был первый случай, когда языковая модель продемонстрировала убедительные признаки emergent abilities и in-context learning на уровне, близком к человеческому.

### Масштаб и архитектура
- 175 млрд параметров, 96 слоёв, 96 голов внимания, d_model = 12 288
- Decoder-only Transformer, идентичный GPT-2 по архитектуре, но увеличенный в 100+ раз
- Обучена на ~45 ТБ очищенного текста (CommonCrawl, WebText2, Books1/2, Wikipedia)

### Три новые парадигмы обучения
1. **Zero-shot** — только инструкция, без примеров
2. **One-shot** — инструкция + один пример
3. **Few-shot** — инструкция + несколько десятков примеров в контексте

Никакого градиентного fine-tuning — всё происходит в одном forward-pass.

### Демонстрации способностей (2020)
| Задача                    | Лучший SOTA 2019–2020 | GPT-3 few-shot |
|---------------------------|-----------------------|----------------|
| TriviaQA                  | 71.2 %                | 64.3 %         |
| LAMBADA (cloze)           | 86.3 %                | 76.2 %         |
| Arithmetic 3-digit        | ~70 %                 | 75–80 %        |
| Перевод En→Fr             | 36–38 BLEU            | 34–35 BLEU     |
| Генерация новостей        | приемлемо             | неотличимо от человека (в слепых тестах) |

### Emergent phenomena
Впервые наблюдались резкие скачки качества при переходе через определённые пороги размера:
- Арифметика многоразрядных чисел
- Простые логические задачи
- Перевод на низкоресурсные языки
- Генерация кода

### Критика и уроки 2020–2025
- Модель не рассуждает, а имитирует рассуждение через шаблоны (позже это исправит Chain-of-Thought)
- Высокая токсичность и сильные биасы из данных CommonCrawl
- Стоимость инференса делала её недоступной большинству исследователей
- Память контекста ограничена 2048 токенами

Тем не менее GPT-3 окончательно доказал гипотезу scaling laws: при достаточном размере и данных языковая модель начинает проявлять способности, которых не было в явном виде в обучающих примерах.

К 2025 году почти все крупные закрытые модели (Claude-3, Gemini Ultra, GPT-4/4o/5) являются прямыми наследниками именно GPT-3, а не GPT-2 или BERT.

См. также: [[Transformer]], [[Attention]]